{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "<img src=\"../../img/ml_theme.png\">\n",
    "# Дополнительное профессиональное <br> образование НИУ ВШЭ\n",
    "#### Программа \"Машинное обучение и майнинг данных\"\n",
    "<img src=\"../../img/faculty_logo.jpg\" height=\"240\" width=\"240\">\n",
    "## Автор материала: преподаватель Факультета Компьютерных Наук НИУ ВШЭ Кашницкий Юрий\n",
    "</center>\n",
    "Материал распространяется на условиях лицензии <a href=\"https://opensource.org/licenses/MS-RL\">Ms-RL</a>. Можно использовать в любых целях, кроме коммерческих, но с обязательным упоминанием автора материала."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Занятие 9. Машинное обучение с Apache Spark\n",
    "## Часть 1. Введение в Apache Spark MLLib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Прдеставление обучающей выборки в Apache Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Методом `parallelize()` уже пользовались"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data = sc.parallelize([[2.11, 5.012, 1.5, 2.6],\n",
    "                       [-21.11, 7.042, 2.5, 2.8]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В MLLib есть плотные и разреженные вектора: **Vectors.dense** и **Vectors.sparse**. На практике в случае больших массивов данных используются разреженные вектора. Чтоб создать разреженный вектор, надо указать общее число элементов и перечислить индексы и значения ненулевых элементов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(DenseVector([2.11, 5.012, 1.5, 2.6]), SparseVector(10, {0: 2.11, 2: 5.012, 5: 1.5, 8: 2.6}))\n"
     ]
    }
   ],
   "source": [
    "from pyspark.mllib.linalg import Vectors\n",
    "dense_vector = Vectors.dense ([2.11, 5.012, 1.5, 2.6])\n",
    "sparse_vector = Vectors.sparse(10, [0,2,5,8], \n",
    "                               [2.11, 5.012, 1.5, 2.6]) # номера ненулевых признаков и их значения\n",
    "print(dense_vector, sparse_vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Для помеченных точек обучающей выборки есть специальный класс **LabeledPoint**\n",
    "- LabeledPoint.features - любой из векторов выше<br>\n",
    "- LabeledPoint.label - имеет тип float и может принимать значения (0.0, 1.0, 2.0 - для классификации, и любое число - для регрессии)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LabeledPoint(1.0, (10,[0,2,5,8],[2.11,5.012,1.5,2.6]))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.mllib.regression import LabeledPoint\n",
    "\n",
    "LabeledPoint(1.0, sparse_vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Загрузка данных в Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Можно сгенерировать **LabeledPoint** из примеров самому"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[LabeledPoint(0.0, [5.1,3.5,1.4,0.2]),\n",
       " LabeledPoint(0.0, [4.9,3.0,1.4,0.2]),\n",
       " LabeledPoint(0.0, [4.7,3.2,1.3,0.2]),\n",
       " LabeledPoint(0.0, [4.6,3.1,1.5,0.2]),\n",
       " LabeledPoint(0.0, [5.0,3.6,1.4,0.2]),\n",
       " LabeledPoint(0.0, [5.4,3.9,1.7,0.4]),\n",
       " LabeledPoint(0.0, [4.6,3.4,1.4,0.3]),\n",
       " LabeledPoint(0.0, [5.0,3.4,1.5,0.2]),\n",
       " LabeledPoint(0.0, [4.4,2.9,1.4,0.2]),\n",
       " LabeledPoint(0.0, [4.9,3.1,1.5,0.1])]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris = sc.textFile (\"../../data/iris.txt\")\n",
    "\n",
    "iris.map(lambda x: [float(v) for v in x.split(\",\")]).map(lambda x: LabeledPoint (x[-1], x[:-1])).take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Для данных в  формате **LibSVM** есть модуль **MLUtils**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[LabeledPoint(-1.0, (8,[0,1,2,3,4,5,6,7],[6.0,148.0,72.0,35.0,0.0,33.599998,0.627,50.0])),\n",
       " LabeledPoint(1.0, (8,[0,1,2,3,4,5,6,7],[1.0,85.0,66.0,29.0,0.0,26.6,0.351,31.0]))]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.mllib.util import MLUtils\n",
    "diabetes = MLUtils.loadLibSVMFile(sc, \"../../data/diabetes.libsvm\")\n",
    "diabetes.take(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Работа с признаками\n",
    "\n",
    "- базовые статистики, распределения,..\n",
    "\n",
    "Прчитаем еще раз данные и отделим признаки от меток целевого класса."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Features: ', [SparseVector(8, {0: 6.0, 1: 148.0, 2: 72.0, 3: 35.0, 4: 0.0, 5: 33.6, 6: 0.627, 7: 50.0}), SparseVector(8, {0: 1.0, 1: 85.0, 2: 66.0, 3: 29.0, 4: 0.0, 5: 26.6, 6: 0.351, 7: 31.0})])\n",
      "('Labels: ', [-1.0, 1.0])\n"
     ]
    }
   ],
   "source": [
    "from pyspark.mllib.util import MLUtils\n",
    "\n",
    "diabetes = MLUtils.loadLibSVMFile(sc, \"../../data/diabetes.libsvm\")\n",
    "\n",
    "features = diabetes.map(lambda x:x.features)\n",
    "labels = diabetes.map(lambda x:x.label)\n",
    "print(\"Features: \", features.take(2))\n",
    "print(\"Labels: \", labels.take(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Есть класс **Statistics** из **pyspark.mllib.stat**, с помощью которого можно получить базовую информацию о признаках."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.mllib.stat import Statistics\n",
    "summary = Statistics.colStats(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  17.      ,  199.      ,  122.      ,   99.      ,  846.      ,\n",
       "         67.099998,    2.42    ,   81.      ])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary.numNonzeros()[:6]\n",
    "summary.mean()\n",
    "summary.min()\n",
    "summary.max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Корреляции"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.        ,  0.12945867,  0.14128198, -0.08167177, -0.07353461,\n",
       "         0.0176831 , -0.03352267,  0.54434123],\n",
       "       [ 0.12945867,  1.        ,  0.15258959,  0.05732789,  0.33135711,\n",
       "         0.22107107,  0.1373373 ,  0.26351432],\n",
       "       [ 0.14128198,  0.15258959,  1.        ,  0.20737054,  0.08893338,\n",
       "         0.28180529,  0.04126495,  0.23952795],\n",
       "       [-0.08167177,  0.05732789,  0.20737054,  1.        ,  0.43678257,\n",
       "         0.3925732 ,  0.18392757, -0.11397026],\n",
       "       [-0.07353461,  0.33135711,  0.08893338,  0.43678257,  1.        ,\n",
       "         0.19785907,  0.18507093, -0.04216295],\n",
       "       [ 0.0176831 ,  0.22107107,  0.28180529,  0.3925732 ,  0.19785907,\n",
       "         1.        ,  0.14064695,  0.03624187],\n",
       "       [-0.03352267,  0.1373373 ,  0.04126495,  0.18392757,  0.18507093,\n",
       "         0.14064695,  1.        ,  0.03356131],\n",
       "       [ 0.54434123,  0.26351432,  0.23952795, -0.11397026, -0.04216295,\n",
       "         0.03624187,  0.03356131,  1.        ]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Statistics.corr(features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Семплирование\n",
    "#### Не входит в MLlib, но это тоже можно делать: как случайные семплы, так и со стратификацией (для устранения перекоса в обучающей выборке)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Обычное сэмплирование. Аналог train_test_split из scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "(train, test) = diabetes.randomSplit([0.8, 0.2], 1344)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Стратифицированные семплы</h4>\n",
    "\n",
    "- когда есть перекос в данных и нужно этот перекос исправить - поэтому это работает только с парами <ключ, значение><br>\n",
    "\n",
    "fractions = {\"a\": 0.5, \"b\": 0.2}<br>\n",
    "RDD.sampleByKey(withReplacement, fractions, seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(<type 'int'>, {1.0: 252, -1.0: 77})"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Выделяем целевую функцию в качестве ключа, считаем частоты\n",
    "diabetes.map(lambda x: (x.label, x)).countByKey()\n",
    "\n",
    "(diabetes\n",
    "    .map (lambda x: (x.label, x))\n",
    "    .sampleByKey(False, {-1:0.25, 1:0.5}, 41)\n",
    "    .countByKey())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Распределения</h3>\n",
    "Можно генерировать случайные распределения: <br>\n",
    "**from pyspark.mllib.random import RandomRDDs** <br><br>\n",
    "- uniformRDD (sc, size, numPartitions, seed) <br>\n",
    "- normalRDD (sc, size, numPartitions, seed) <br>\n",
    "- poissonRDD (sc, size, numPartitions, seed) <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Масштабирование признаков</h3> \n",
    "\n",
    "Особенно важно в алгоритмах, в которых используется градиентный спуск (лог-регрессия, SVM, бустинг). Класс **StandardScaler** работает только с плотными векторами (!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[DenseVector([0.6395, 0.8478, 0.1495, 0.9067, -0.6924, 0.2039, 0.4682, 1.4251]),\n",
       " DenseVector([-0.8443, -1.1227, -0.1604, 0.5306, -0.6924, -0.684, -0.3648, -0.1905]),\n",
       " DenseVector([1.2331, 1.9425, -0.2638, -1.2874, -0.6924, -1.1025, 0.604, -0.1055]),\n",
       " DenseVector([-0.8443, -0.9976, -0.1604, 0.1544, 0.1232, -0.4937, -0.9202, -1.0409]),\n",
       " DenseVector([-1.1411, 0.5037, -1.5037, 0.9067, 0.7653, 1.4088, 5.4813, -0.0205]),\n",
       " DenseVector([0.3428, -0.1531, 0.2529, -1.2874, -0.6924, -0.8108, -0.8175, -0.2756]),\n",
       " DenseVector([-0.2508, -1.3416, -0.9871, 0.7186, 0.0712, -0.1259, -0.6757, -0.6157]),\n",
       " DenseVector([1.8266, -0.1844, -3.5703, -1.2874, -0.6924, 0.4195, -1.0198, -0.3606]),\n",
       " DenseVector([-0.5476, 2.3803, 0.0462, 1.5336, 4.0193, -0.1893, -0.9473, 1.6802]),\n",
       " DenseVector([1.2331, 0.1284, 1.3895, -1.2874, -0.6924, -4.0578, -0.724, 1.7652])]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.mllib.feature import StandardScaler\n",
    "\n",
    "scaler1 = StandardScaler(withMean=True, withStd=True).fit(features)\n",
    "\n",
    "scaler1.transform (features.map(lambda x:x.toArray())).take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Классификация и регрессия</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.mllib.util import MLUtils\n",
    "diabetes = MLUtils.loadLibSVMFile(sc, \"../../data/diabetes.libsvm\")\n",
    "\n",
    "# делим выборку на обучающую и тестовую\n",
    "(train, test) = diabetes.randomSplit([0.8, 0.2], seed=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[LabeledPoint(1.0, (8,[0,1,2,3,4,5,6,7],[1.0,85.0,66.0,29.0,0.0,26.6,0.351,31.0])),\n",
       " LabeledPoint(-1.0, (8,[0,1,2,3,4,5,6,7],[8.0,183.0,64.0,0.0,0.0,23.299999,0.672,32.0])),\n",
       " LabeledPoint(1.0, (8,[0,1,2,3,4,5,6,7],[1.0,89.0,66.0,23.0,94.0,28.1,0.167,21.0])),\n",
       " LabeledPoint(-1.0, (8,[0,1,2,3,4,5,6,7],[0.0,137.0,40.0,35.0,168.0,43.099998,2.288,33.0])),\n",
       " LabeledPoint(1.0, (8,[0,1,2,3,4,5,6,7],[5.0,116.0,74.0,0.0,0.0,25.6,0.201,30.0]))]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Линейные классификаторы</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В линейных классификаторах решается по сути задача оптимизации:<br>\n",
    "<br>\n",
    "$$f(w) = \\lambda R(w) + \\frac{1}{n} \\sum_{i=1}^n L(w;x_i,y_i) $$<br>\n",
    "<br>\n",
    "$L(w;x,y)$ - функция потерь<br>\n",
    "$R(w)$ - функция регуляризации<br><br>\n",
    "- Для SVM: $L(w;x,y) = \\max\\{0,1-yw^Tx\\}, y \\in \\{-1, +1\\}$<br><br>\n",
    "- Для логистической регрессии: $L(w;x,y) = log(1+\\exp(-yw^Tx)), y \\in \\{-1,+1\\}$<br><br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В Mllib все делается методом стохастического градиентного спуска SGD:<br>\n",
    " - pyspark.mllib.classification.SVMWithSGD<br>\n",
    " - pyspark.mllib.classification.LogisticRegressionWithSGD<br>\n",
    "<br><br>\n",
    "**classifier.train(trainDataset)** - обучает модель, среди параметров, как и в scikit-learn:<br>\n",
    "- data – обучающая выборка RDD of LabeledPoint.<br>\n",
    "- iterations – число итераций (по умолчанию: 100).<br>\n",
    "- step – шаг SGD (по умолчанию: 1.0).<br>\n",
    "- miniBatchFraction – доля данных которая используется на каждой SGD итерации<br>\n",
    "- initialWeights – начальные веса (по умолчанию None)<br>\n",
    "- regParam – параметр регуляризации (по умолчанию: 0.01)<br>\n",
    "- regType – тип регуляризатора (по умолчанию \"l2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Градиентный спуск**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "При стандартном (или «пакетном», «batch») градиентном спуске для корректировки параметров модели используется градиент. Градиент обычно считается как сумма градиентов, вызванных каждым элементом обучения. Вектор параметров изменяется в направлении антиградиента с заданным шагом. Поэтому стандартному градиентному спуску требуется один проход по обучающим данным до того, как он сможет менять параметры."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='../../img/grad_descent.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Стохастический градиентный спуск**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "При стохастическом (или «оперативном») градиентном спуске значение градиента аппроксимируются градиентом функции потерь, вычисленном только на одном элементе обучения. \n",
    "То есть а каждом шаге случайно выбирается $x_i$, и \"шаг\" делается в сторону уменьшения функции по переменной $x_i$. \n",
    "Затем параметры изменяются пропорционально приближенному градиенту. Таким образом, параметры модели изменяются после каждого объекта обучения. Для больших массивов данных стохастический градиентный спуск может дать значительное преимущество в скорости по сравнению со стандартным градиентным спуском.\n",
    "Между этими двумя видами градиентного спуска существует компромисс, называемый иногда «mini-batch». В этом случае градиент аппроксимируется суммой для небольшого количества обучающих примеров.\n",
    "Стохастический градиентный спуск является одной из форм стохастического приближения. Теория стохастических приближений даёт условия сходимости метода стохастического градиентного спуска."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='../../img/stochastic.png'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Accuracy: ', 0.62666666666666671)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pyspark.mllib.classification as cl\n",
    "\n",
    "def acc_score(actual, predicted):\n",
    "    actual, predicted = np.array(actual), np.array(predicted)\n",
    "    return np.sum(actual == predicted) / float(actual.shape[0])\n",
    "\n",
    "def change_label(lp):\n",
    "    \"\"\"\n",
    "    Substitute label -1 to 0 for Logit.\n",
    "    \"\"\"\n",
    "    label, features = lp.label, lp.features\n",
    "    if label == -1:\n",
    "        label = 0 \n",
    "    return LabeledPoint(label, features)\n",
    "\n",
    "diabet_train, diabet_test = diabetes.map(lambda lp: change_label(lp)).randomSplit([0.8, 0.2], seed=13)\n",
    "diabet_test_labels = diabet_test.map(lambda lp: lp.label).collect()\n",
    "model = cl.LogisticRegressionWithSGD.train(diabet_train, iterations=500, regType=\"l1\")\n",
    "diabet_test_predicted = diabet_test.map(lambda x: model.predict(x.features)).collect()\n",
    "print(\"Accuracy: \", acc_score(diabet_test_labels, diabet_test_predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Линейная регрессия</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Тут, фактически, решается та же задача оптимизации, только с другой функцией потерь:<br>\n",
    "<br>\n",
    "$$\\Large f(w) = \\lambda R(w) + \\frac{1}{n} \\sum_{i=1}^n L(w;x_i,y_i) $$<br>\n",
    "\n",
    "$$\\Large L(w;x,y) = \\frac{1}{2} (w^Tx-y)^2, y \\in \\mathbb{R}$$<br>\n",
    "\n",
    "- Если $R(w)=0$, то это **линейная регрессия** - LinearRegressionWithSGD\n",
    "- Если $R(w)=||w||_2$ - **лассо-регерессия** -  LassoWithSGD\n",
    "- Если $R(w)=\\frac{1}{2}||w||_2^2$ - **Ridge-регрессия** - RidgeRegressionWithSGD\n",
    "\n",
    "Параметры такие же, как и у линейных классификаторов (см. выше)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pyspark.mllib.regression as reg\n",
    "model = reg.RidgeRegressionWithSGD.train(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Наивный Байес</h3>\n",
    "\n",
    "Наивный Баейс принимает на вход всего **2** параметра: **обучающую выборку** и **параметр сглаживания**<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Accuracy: ', 0.56666666666666665)\n"
     ]
    }
   ],
   "source": [
    "from pyspark.mllib.classification import NaiveBayes\n",
    "\n",
    "model = NaiveBayes.train(diabet_train)\n",
    "diabet_test_predicted = diabet_test.map(lambda x: model.predict(x.features)).collect()\n",
    "print(\"Accuracy: \", acc_score(diabet_test_labels, diabet_test_predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Дерево решений\n",
    "\n",
    "В Spark это **DecisionTree.trainClassifier** и **DecisionTree.trainRegressor**\n",
    "\n",
    "- **data** – обучающая выборка, RDD\n",
    "- **numClasses** – число классов (только для классификатора)\n",
    "- **categoricalFeaturesInfo** – указание категорий для категориальных признаков\n",
    "- **impurity** – критерий качества разбиения. Значения:\n",
    " - “gini” (для классификации, рекомендуется)\n",
    " - “entropy” (для классификации)\n",
    " - \"variance\" (для регрессии)\n",
    "- **maxDepth** – макс глубина дерева\n",
    "- ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Accuracy: ', 0.66000000000000003)\n"
     ]
    }
   ],
   "source": [
    "from pyspark.mllib.tree import DecisionTree\n",
    "\n",
    "model = DecisionTree.trainClassifier(diabet_train, numClasses=2, \n",
    "                                     categoricalFeaturesInfo={}, \n",
    "                                     impurity='entropy', \n",
    "                                     maxDepth=5)\n",
    "diabet_test_predicted = model.predict(diabet_test.map(lambda lp: lp.features))\n",
    "print(\"Accuracy: \", acc_score(diabet_test_labels, diabet_test_predicted.collect()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Случайный лес\n",
    "\n",
    "- **trainClassifier** для обучения классификатора \n",
    "- **trainRegression** для обучения функции регрессии\n",
    "\n",
    "Параметры:\n",
    "\n",
    "- **data** – обучающая выборка, RDD LabeledPoint. Классов может быть несколько\n",
    "- **numClasses** – число классов для классификации\n",
    "- **categoricalFeaturesInfo** – информация по категориальным признакам. Запись (n -> k) показывает, что признак n - категориальный и имеет k категорий, начинащихся с 0: $\\{0, 1, ..., k-1\\}$.\n",
    "- **numTrees** – число деревьев в лесу\n",
    "- **featureSubsetStrategy** - число признаков, которые рассматриваются для разбиения в каждом узле. Возможные варианты:\n",
    " - **“auto”** (по умолчанию, если одно дерево - выбираются все признаки, иначе - берется корень из числа признаков)\n",
    " - **“all”**\n",
    " - **“sqrt”**\n",
    " - **“log2”**\n",
    " - **“onethird”**\n",
    " \n",
    "- **impurity** – критерий качества разбиения. Значения:\n",
    " - “gini” (для классификации, рекомендуется)\n",
    " - “entropy” (для классификации)\n",
    " - \"variance\" (для регрессии)\n",
    "- **maxDepth** – максимальная глубина дерева\n",
    "- **maxBins** – максимальное число корзин, используемое для разделения признаков (по умолчанию 100)\n",
    "- **seed** – для воспроизводимой случайности bootstrap и выбора подмножества признаков"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.mllib.tree import RandomForest\n",
    "model = RandomForest.trainClassifier(diabet_train, numClasses=2, \n",
    "                                     categoricalFeaturesInfo={}, numTrees = 100, \n",
    "                                     featureSubsetStrategy=\"auto\",\n",
    "                                     impurity='gini', maxDepth=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Accuracy: ', 0.69333333333333336)\n"
     ]
    }
   ],
   "source": [
    "diabet_test_predicted = model.predict(diabet_test.map(lambda lp: lp.features))\n",
    "print(\"Accuracy: \", acc_score(diabet_test_labels, diabet_test_predicted.collect()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Кластеризация KMeans\n",
    "\n",
    "Обучение модели - KMeans.train с параметрами\n",
    "- **dataset** - датасет векторов\n",
    "- **k** - число кластеров\n",
    "- **maxIterations** - число итераций сходимости\n",
    "- **initializationMode** - дотупные варианты \"k-means||\" (по умолчанию), \"random\"\n",
    "- **runs** - (по умолчанию 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.mllib.clustering import KMeans\n",
    "\n",
    "iris = sc.textFile (\"../../data/iris.txt\")\n",
    "\n",
    "iris_features ,iris_labels = iris.map(lambda x: Vectors.dense([float(v) for v in x.split(\",\")[:-1]])), \\\n",
    "                            iris.map(lambda x: int(x.split(\",\")[-1])).collect()\n",
    "\n",
    "# Build the model (cluster the data)\n",
    "clusters = KMeans.train(iris_features, 3, maxIterations = 100,\n",
    "        runs=10, initializationMode=\"random\")\n",
    "\n",
    "cluster_labels = iris_features.map(lambda vec: clusters.predict(vec)).collect()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  },
  "name": "lesson11_part5_MLLib_intro.ipynb"
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
