{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "<img src=\"../../img/ml_theme.png\">\n",
    "# Дополнительное профессиональное <br> образование НИУ ВШЭ\n",
    "#### Программа \"Машинное обучение и майнинг данных\"\n",
    "<img src=\"../../img/faculty_logo.jpg\" height=\"240\" width=\"240\">\n",
    "## Автор материала: преподаватель Факультета Компьютерных Наук НИУ ВШЭ Кашницкий Юрий\n",
    "</center>\n",
    "Материал распространяется на условиях лицензии <a href=\"https://opensource.org/licenses/MS-RL\">Ms-RL</a>. Можно использовать в любых целях, кроме коммерческих, но с обязательным упоминанием автора материала."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Занятие 8. Apache Spark\n",
    "## Часть 3. Подсчет числа слов в документе"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#![Spark Logo](http://spark-mooc.github.io/web-assets/images/ta_Spark-logo-small.png) + ![Python Logo](http://spark-mooc.github.io/web-assets/images/python-logo-master-v3-TM-flattened_small.png)\n",
    "#### Подсчитаем самые частые слова в  [полном собрании сочинений Уильяма Шекспира ](http://www.gutenberg.org/ebooks/100)([проект Гутенберга](http://www.gutenberg.org/wiki/Main_Page)).  \n",
    "#### **План: **\n",
    "#### *Часть 1:* Создание RDD и парных RDD\n",
    "#### *Часть 2:* Подсчет слов с помощью парных RDD\n",
    "#### *Часть 3:* Нахождение уникальных слов\n",
    "#### *Часть 4:* Подсчет слов в файле"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ** Часть 1: Создание RDD и парных RDD**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ** (1a) Создание RDD **\n",
    "#### Метод `sc.parallelize`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pyspark.rdd.RDD'>\n"
     ]
    }
   ],
   "source": [
    "wordsList = ['cat', 'elephant', 'rat', 'rat', 'cat']\n",
    "wordsRDD = sc.parallelize(wordsList, 4)\n",
    "# Print out the type of wordsRDD\n",
    "print(type(wordsRDD))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ** (1b) Функция map **\n",
    "#### Напишем функцию и используем map(), чтобы применить ее ко всему набору данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cats\n"
     ]
    }
   ],
   "source": [
    "def makePlural(word):\n",
    "    \"\"\"Adds an 's' to `word`.\n",
    "\n",
    "    Note:\n",
    "        This is a simple function that only adds an 's'.  No attempt is made to follow proper\n",
    "        pluralization rules.\n",
    "\n",
    "    Args:\n",
    "        word (str): A string.\n",
    "\n",
    "    Returns:\n",
    "        str: A string with 's' added to it.\n",
    "    \"\"\"\n",
    "    return word + 's'\n",
    "\n",
    "print(makePlural('cat'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ** (1c) Применение `makePlural` к RDD **\n",
    "#### Используем [map()](http://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD.map) и [collect()](http://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD.collect)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['cats', 'elephants', 'rats', 'rats', 'cats']\n"
     ]
    }
   ],
   "source": [
    "pluralRDD = wordsRDD.map(makePlural)\n",
    "print(pluralRDD.collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ** (1d)  `map` с  `lambda`-функцией **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['cats', 'elephants', 'rats', 'rats', 'cats']\n"
     ]
    }
   ],
   "source": [
    "pluralLambdaRDD = wordsRDD.map(lambda word: word + 's')\n",
    "print(pluralLambdaRDD.collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ** (1e) Длины слов **\n",
    "#### Используем `map()` и `lambda`-функцию для подсчета длины каждого слова"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pluralLengths = (pluralRDD\n",
    "                 .map(lambda word: len(word))\n",
    "                 .collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ** (1f) Парные RDD **\n",
    "#### Парный RDD - это такой RDD, каждый элемент которого - кортеж `(k, v)`  - \"ключ-значение\". Здесь мы создаем пары (<слово>, 1), чтобы потом просуммировать все единицы. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('cat', 1), ('elephant', 1), ('rat', 1), ('rat', 1), ('cat', 1)]\n"
     ]
    }
   ],
   "source": [
    "wordPairs = wordsRDD.map(lambda word: (word, 1))\n",
    "print wordPairs.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ** Часть 2: Подсчет слов с помощью парных RDD**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ** (2a) Подход на основе `groupByKey()`  **\n",
    "#### Здесь используем преобразование [groupByKey()](http://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD.groupByKey). Порождаются пары вида `('<слово>', <итератор>)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rat: [1, 1]\n",
      "elephant: [1]\n",
      "cat: [1, 1]\n"
     ]
    }
   ],
   "source": [
    "wordsGrouped = wordPairs.groupByKey()\n",
    "for key, value in wordsGrouped.collect():\n",
    "    print('{0}: {1}'.format(key, list(value)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ** (2b)  `groupByKey()` для подсчета числа слов **\n",
    "#### Теперь суммируем с помощью преобразования `map()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('rat', 2), ('elephant', 1), ('cat', 2)]\n"
     ]
    }
   ],
   "source": [
    "wordCountsGrouped = wordsGrouped.map(lambda (word, ones): (word, sum(ones)))\n",
    "print wordCountsGrouped.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ** (2c) Подсчет с помощью `reduceByKey` **\n",
    "#### Лучше использовать метод  [reduceByKey()](http://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD.reduceByKey), который объединяет кортежи с одинаковым ключом и применяет к ним некоторую функцию. И так итеративно, пока каждому ключу не будет соответствовать ровно одно значение. Метод легко масштабируется, поскольку указанную операцию можно выполнять распределенно на разных машинах."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('rat', 2), ('elephant', 1), ('cat', 2)]\n"
     ]
    }
   ],
   "source": [
    "# Note that reduceByKey takes in a function that accepts two values and returns a single value\n",
    "wordCounts = wordPairs.reduceByKey(lambda a, b: a + b)\n",
    "print(wordCounts.collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ** (2d) Все вместе **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('rat', 2), ('elephant', 1), ('cat', 2)]\n"
     ]
    }
   ],
   "source": [
    "wordCountsCollected = (wordsRDD\n",
    "                       .map(lambda word: (word, 1))\n",
    "                       .reduceByKey(lambda a,b: a + b)\n",
    "                       .collect())\n",
    "print(wordCountsCollected)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ** Part 3: Нахождение уникальных слов **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ** (3a) Уникальные слова **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "uniqueWords = wordCounts.count()\n",
    "print(uniqueWords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ** (3b) Метод `reduce` **\n",
    "#### Среднее число появлений каждого уникального слова"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "1.67\n"
     ]
    }
   ],
   "source": [
    "from operator import add\n",
    "totalCount = (wordCounts\n",
    "              .map(lambda (word, count): count)\n",
    "              .reduce(add))\n",
    "average = totalCount / float(wordCounts.count())\n",
    "print(totalCount)\n",
    "print(round(average, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ** Часть 4: Подсчет числа слов в файле **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Завершим создание функции для подсчета слов в файле. Учтем регистр слов и пунктуацию."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ** (4a) Функция `wordCount`  **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('rat', 2), ('elephant', 1), ('cat', 2)]\n"
     ]
    }
   ],
   "source": [
    "def wordCount(wordListRDD):\n",
    "    \"\"\"Creates a pair RDD with word counts from an RDD of words.\n",
    "\n",
    "    Args:\n",
    "        wordListRDD (RDD of str): An RDD consisting of words.\n",
    "\n",
    "    Returns:\n",
    "        RDD of (str, int): An RDD consisting of (word, count) tuples.\n",
    "    \"\"\"\n",
    "    return wordListRDD.map(lambda word: (word, 1)).reduceByKey(add)\n",
    "\n",
    "print(wordCount(wordsRDD).collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ** (4b) Регистр и пунктуация **\n",
    " \n",
    "#### Функция `removePunctuation` приводит все слова к нижнему регистру, удаляет пунктуацию и пробелы в начале и конце сло.  Используем модуль [re](https://docs.python.org/2/library/re.html) для удаления любых символов, отличных от букв, чисел и знака пробела."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hi you\n",
      "no underscore\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def removePunctuation(text):\n",
    "    \"\"\"Removes punctuation, changes to lower case, and strips leading and trailing spaces.\n",
    "\n",
    "    Note:\n",
    "        Only spaces, letters, and numbers should be retained.  Other characters should should be\n",
    "        eliminated (e.g. it's becomes its).  Leading and trailing spaces should be removed after\n",
    "        punctuation is removed.\n",
    "\n",
    "    Args:\n",
    "        text (str): A string.\n",
    "\n",
    "    Returns:\n",
    "        str: The cleaned up string.\n",
    "    \"\"\"\n",
    "    return re.sub('[^a-zA-Z 0-9]','',text.lower().strip())\n",
    "\n",
    "print(removePunctuation('Hi, you!'))\n",
    "print(removePunctuation(' No under_score!'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ** (4c) Загрузка файла **\n",
    "#### Для превращения текстового файла в RDD используем метод `SparkContext.textFile()` . "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: 1609\n",
      "1: \n",
      "2: the sonnets\n",
      "3: \n",
      "4: by william shakespeare\n",
      "5: \n",
      "6: \n",
      "7: \n",
      "8: 1\n",
      "9: from fairest creatures we desire increase\n",
      "10: that thereby beautys rose might never die\n",
      "11: but as the riper should by time decease\n",
      "12: his tender heir might bear his memory\n",
      "13: but thou contracted to thine own bright eyes\n",
      "14: feedst thy lights flame with selfsubstantial fuel\n"
     ]
    }
   ],
   "source": [
    "shakespeareRDD = (sc\n",
    "                  .textFile('../data/shakespeare.txt')\n",
    "                  .map(removePunctuation))\n",
    "print('\\n'.join(shakespeareRDD\n",
    "                .zipWithIndex()  # to (line, lineNum)\n",
    "                .map(lambda (l, num): '{0}: {1}'.format(num, l))  # to 'lineNum: line'\n",
    "                .take(15)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ** (4d) Извлечение слов из строк **\n",
    "#### Подзадачи:\n",
    "  + #### Каждую строку разбиваем по пробелам;\n",
    "  + #### Удаляем пустые строки."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'zwaggerd', u'zounds', u'zounds', u'zounds', u'zounds']\n",
      "928908\n"
     ]
    }
   ],
   "source": [
    "shakespeareWordsRDD = shakespeareRDD.flatMap(lambda s: s.split(' '))\n",
    "shakespeareWordCount = shakespeareWordsRDD.count()\n",
    "print(shakespeareWordsRDD.top(5))\n",
    "print(shakespeareWordCount)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ** (4e) Удаление \"пустых\" слов  **\n",
    "#### Используем метод `filter`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "882996\n"
     ]
    }
   ],
   "source": [
    "shakeWordsRDD = shakespeareWordsRDD.filter(lambda elem: elem != '')\n",
    "shakeWordCount = shakeWordsRDD.count()\n",
    "print(shakeWordCount)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ** (4f) Подсчет слов **\n",
    "#### Применяем функцию `wordCount()`, затем  `takeOrdered()` для выведения самых частых слов. Для `takeOrdered()` нужна своя функция сортировки, поскольку каждый элемент `shakeWordsRDD` - это кортеж. \n",
    "#### Появляются стоп-слова. Чаще всего при анализе текстов их игнорируют. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(u'the', 27361), (u'and', 26028), (u'i', 20681), (u'to', 19150), (u'of', 17463), (u'a', 14593), (u'you', 13615), (u'my', 12481), (u'in', 10956), (u'that', 10890), (u'is', 9134), (u'not', 8497), (u'with', 7771), (u'me', 7769), (u'it', 7678)]\n",
      "the: 27361\n",
      "and: 26028\n",
      "i: 20681\n",
      "to: 19150\n",
      "of: 17463\n",
      "a: 14593\n",
      "you: 13615\n",
      "my: 12481\n",
      "in: 10956\n",
      "that: 10890\n",
      "is: 9134\n",
      "not: 8497\n",
      "with: 7771\n",
      "me: 7769\n",
      "it: 7678\n"
     ]
    }
   ],
   "source": [
    "top15WordsAndCounts = (shakeWordsRDD\n",
    "                       .map(lambda word: (word, 1))\n",
    "                       .reduceByKey(add)\n",
    "                       .takeOrdered(15, key=lambda (w, c): -c))\n",
    "print(top15WordsAndCounts)\n",
    "print('\\n'.join(map(lambda (w, c): '{0}: {1}'.format(w, c), top15WordsAndCounts)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
